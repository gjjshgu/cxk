# QG训练营xx组第二周周记：
2020年4月4日

## 生活随记
- 也没什么可以记吧，压力倒是挺大的，2号晚上的训练营作业居然今天就要交了，况且相比难度比链表大很多，不过我个人感觉也还好吧，计算器那里时间花得多一点。
- 沙特和俄罗斯在特靠谱的号召下，原油减产一千万桶，油价瞬间拉起，我30成本的布伦特原油涨到34了哈哈哈，继续涨继续涨！
- 转专业报名出来了，准备去报名，希望能转。化学专业太难顶了，高数学b类（学一本半），线代大二才上，很影响我推算法啊！所以压力是真的大，线代是自学的，拉格朗日乘子法也是自学的，希望转过去吧。
- 好多人都很努力呀，特别qw他二号晚上就把计算器搞定了，是真的强，我是个遇强则强的人，别人的强大给予我无穷的动力，我要凭自己的努力刷掉各位大佬！加油，奥利给！
## 一周总结
- 对线性判别分析（LDA）进行了推理与手动实现。推理后得到的模型为
	$$y=W^Tx+b$$
    其中$b=\frac{1}{2}W^T(\bar x_{c1}-\bar x_{c2})$，而$W\propto S_w^{-1}(\bar x_{c1}-\bar x_{c2})$。
    推导过程笔记我的GitHub上面有，就在放训练营任务的根目录中。
- 多分类逻辑回归的实现
  - one vs all：比如一个数据集的target为三类，分别是0，1，2，那么one vs all的过程就是建立三个模型，
  首先把{0}和{1,2}分为一类训练，接着把{1}和{0,2}分为另一类，最后把{2}和{0,1}分为一类。用这三个对数据集进行预测。
  缺点：one vs all 可能是因为每个类别的数据并不多，然后又出现多个分类的情况，这样的话可能会导致精确度下降的风险。
  - one vs one：又称为softmax回归，模型为：
    $$P(c=k|x,\theta)=\frac{\exp(\theta_k^Tx)}{\sum_{i=1}^K\exp(\theta_k^Tx)}$$
    其对数似然函数为：
    $$J_m(\theta)=\sum_{i=1}^m\sum_{k=1}^Ky_k^{(i)}-\log\sum_{l=1}^K\exp(\theta_l^Tx^{(i)})$$
    $$J(\theta)=\sum_{k=1}^Ky_k*(\theta_k^Tx^{(i)}-\log\sum_{l=1}^Kexp(\theta_k^Tx^{(i)}))$$
    对其求偏导，为：
    $$\frac{∂J(\theta)}{∂\theta_k}=(y_k-\frac{\theta_k^Tx}{\sum_{i=1}^K\exp(\theta_k^Tx)})*x$$
    使用梯度下降法对参数$\theta$优化。我还没有对Softmax回归作出推导，所以只能做做笔记了。
    ```python
    class Softmax:
    def __init__(self):
        self.weight = None

    def fit(self, X_train, y_train, n_iters=100, alpha=0.1):
        def softmax(X):
            return np.exp(X) / np.sum(np.exp(X))

        target_nums = len(set(y_train))
        self.weight = np.random.random((target_nums, X_train.shape[1]))
        for _ in range(n_iters):
            for i in range(len(X_train)):
                x = X_train[i].reshape(-1, 1)
                y = np.zeros((target_nums, 1))
                y[y_train[i]] = 1
                _y = softmax(np.dot(self.weight, x))
                a = (np.dot((_y - y), x.T))
                self.weight -= alpha * a

    def predict(self, X_test):
        return np.argmax(self.weight.dot(X_test.T), axis=0)
    ```
- kd树的实现
  kd树的划分，其实就是对目标维度取中位数，小于中位数的分到左边，大于的分到右边，一直递归下去，直到分不了了就停止。目标维度与层数的关系为`next_axis = (axis + 1) % k_dimensions`。
  代码如下：
  ```python
    class KDTree:
    def __init__(self, X, y=None):
        self.root = None
        self.y_exist = False if y is None else True
        self.create(X, y)

    def create(self, X, y=None):
        def _create(X, axis, parent=None):
            nums = X.shape[0]
            if nums == 0: return None
            mid = (nums >> 1)
            index = np.argsort(X[:, axis])
            small = np.array([X[i] for i in index[:mid]])
            big = np.array([X[i] for i in index[mid+1:]])
            if self.y_exist:
                kd_node = KDNode(X[index[mid]][:-1], X[mid][-1], axis=axis, parent=parent)
            else:
                kd_node = KDNode(X[index[mid]], axis=axis, parent=parent)

            next_axis = (axis + 1) % k_dimensions
            kd_node.left = _create(small, next_axis, kd_node)
            kd_node.right = _create(big, next_axis, kd_node)
            return kd_node

        k_dimensions = X.shape[1]
        print("正在创建KD树......")
        if y is not None:
            X = np.hstack((X, y.reshape(-1, 1)))
        self.root = _create(X, 0)
        print("创建完毕")
  ```
## 存在问题
1. 完成训练营任务的速度还是有点慢吧，qw用了一个晚上就完成的任务我居然花了一天！还是我太菜了，但是这不代表我永远都这么菜！
2. 推导还是有点吃力，数学学得太慢了啊！
3. 任务比较多，要合理分配时间。
## 下周规划
1. 1天内完成下次训练营任务
2. 完成研发一轮考核任务
3. 完成。。学校作业
4. 为转专业做准备！